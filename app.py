import streamlit as st
import tweepy
import re
import pandas as pd
import requests
import time
import plotly.express as px
from datetime import datetime, timedelta

st.set_page_config(page_title="MVP Clima en X", layout="wide")
st.title("ğŸ“Š MVP â€“ Clima del Tema en X")

bearer_token = st.secrets["X_BEARER_TOKEN"]
client = tweepy.Client(bearer_token=bearer_token)

st.success("Conectado a X correctamente âœ…")

query = st.text_input("Palabras clave / hashtags")

time_range = st.selectbox(
    "Rango temporal",
    ["24 horas", "7 dÃ­as", "30 dÃ­as"]
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Selector de modelo de sentimiento (Hugging Face)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODELOS_SENTIMIENTO = {
    "BETO (ES) â€“ recomendado": "finiteautomata/beto-sentiment-analysis",
    "Robertuito (ES) â€“ social": "pysentimiento/robertuito-sentiment-analysis",
    "Twitter-RoBERTa (X) â€“ actual": "cardiffnlp/twitter-roberta-base-sentiment-latest",
}

modelo_nombre = st.selectbox(
    "Modelo de sentimiento (IA)",
    list(MODELOS_SENTIMIENTO.keys()),
    index=0
)

modelo_hf_id = MODELOS_SENTIMIENTO[modelo_nombre]
HF_MODEL_URL = f"https://router.huggingface.co/hf-inference/models/{modelo_hf_id}"


# Lista simple (MVP) de departamentos/ciudades clave para inferir ubicaciÃ³n
PERU_PLACES = [
    "Amazonas","Ãncash","ApurÃ­mac","Arequipa","Ayacucho","Cajamarca","Callao","Cusco",
    "Huancavelica","HuÃ¡nuco","Ica","JunÃ­n","La Libertad","Lambayeque","Lima","Loreto",
    "Madre de Dios","Moquegua","Pasco","Piura","Puno","San MartÃ­n","Tacna","Tumbes","Ucayali",
    # Ciudades muy usadas en perfil
    "Trujillo","Chiclayo","Huancayo","Iquitos","Tarapoto","Pucallpa","Juliaca"
]

def get_start_time(option):
    if option == "24 horas":
        return datetime.utcnow() - timedelta(hours=24)
    if option == "7 dÃ­as":
        return datetime.utcnow() - timedelta(days=7)
    return datetime.utcnow() - timedelta(days=30)

def infer_peru_location(profile_location: str, profile_desc: str):
    
    """
    Inferencia Ã©tica y simple:
    - Usa 'location' del perfil (si existe)
    - Busca menciones a lugares de PerÃº
    - Devuelve (ubicacion_inferida, confianza, fuente)
    """
    loc = (profile_location or "").strip()
    desc = (profile_desc or "").strip()

    # Normalizamos texto para comparar
    haystack = f"{loc} {desc}".lower()

    # SeÃ±ales de PerÃº
    peru_signals = ["perÃº", "peru", "ğŸ‡µğŸ‡ª", "lima", "cusco", "arequipa", "piura", "callao"]
    mentions_peru = any(s in haystack for s in peru_signals)

    # Buscar match exacto (case-insensitive) de lista
    for place in PERU_PLACES:
        if re.search(rf"\b{re.escape(place.lower())}\b", haystack):
            # Confianza:
            # - Media si viene del campo location del perfil
            # - Baja si viene solo de la bio/description
            if loc and place.lower() in loc.lower():
                return place, "Media", "Perfil (location)"
            return place, "Baja", "Bio/DescripciÃ³n"

    # Si solo dice "PerÃº" sin regiÃ³n
    if loc and ("perÃº" in loc.lower() or "peru" in loc.lower() or "ğŸ‡µğŸ‡ª" in loc):
        return "PerÃº (sin regiÃ³n)", "Baja", "Perfil (location)"

    # Sin datos
    if not loc and not desc:
        return "No disponible", "N/A", "Sin datos"

    # Algo hay, pero no identificamos regiÃ³n
    if mentions_peru:
        return "PerÃº (no identificada)", "Baja", "SeÃ±ales en perfil/bio"
    return "No inferible", "N/A", "Sin seÃ±ales claras"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Sentimiento con Hugging Face (CardiffNLP Twitter-RoBERTa)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def sentimiento_hf(texto: str):
    """
    Devuelve: (sentimiento, score)
    - sentimiento: Positivo / Neutral / Negativo o None
    - score: confianza 0..1 o None
    """
    HF_TOKEN = st.secrets.get("HF_TOKEN", "")
    if not HF_TOKEN:
        return None, None

    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    payload = {"inputs": texto[:512]}

    try:
        r = requests.post(HF_MODEL_URL, headers=headers, json=payload, timeout=25)
        if r.status_code != 200:
            return None, None

        data = r.json()

        # A veces viene [[...]]
        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], list):
            data = data[0]

        if not isinstance(data, list) or len(data) == 0:
            return None, None

        best = max(data, key=lambda x: x.get("score", 0))
        label = best.get("label", "")
        score = best.get("score", 0)

        mapping = {
            "positive": "Positivo",
            "neutral": "Neutral",
            "negative": "Negativo",
            "pos": "Positivo",
            "neu": "Neutral",
            "neg": "Negativo",
            "LABEL_2": "Positivo",
            "LABEL_1": "Neutral",
            "LABEL_0": "Negativo",
        }

        sentimiento = mapping.get(label.lower(), mapping.get(label, None))
        if sentimiento is None:
            return None, None

        return sentimiento, round(float(score), 3)

    except Exception:
        return None, None

if "last_search_ts" not in st.session_state:
    st.session_state["last_search_ts"] = 0

if st.button("Buscar en X"):
    now = time.time()
    if now - st.session_state["last_search_ts"] < 20:
        st.warning("Espera 20 segundos entre bÃºsquedas para evitar lÃ­mites de X.")
        st.stop()
    st.session_state["last_search_ts"] = now

    if not query:
        st.warning("Ingresa una palabra clave")
    else:
        start_time = get_start_time(time_range).isoformat("T") + "Z"

        # Pedimos tambiÃ©n info del autor vÃ­a expansions
        try:
            response = client.search_recent_tweets(
                query=query,
                start_time=start_time,
                max_results=50,
                tweet_fields=["created_at", "public_metrics", "author_id"],
                expansions=["author_id"],
                user_fields=["username", "name", "location", "description"]
            )
        except tweepy.errors.TooManyRequests as e:
            # Intentar leer "reset time" si existe
            reset_info = ""
            try:
                reset_ts = int(e.response.headers.get("x-rate-limit-reset", "0"))
                if reset_ts:
                    wait_sec = max(0, reset_ts - int(time.time()))
                    wait_min = max(1, int(round(wait_sec / 60)))
                    reset_info = f"â³ Intenta nuevamente en ~{wait_min} min."
            except Exception:
                pass
        
            st.error(
                "âš ï¸ LÃ­mite de consultas alcanzado en la API de X (rate limit).\n\n"
                "Esto ocurre cuando se hacen varias bÃºsquedas en poco tiempo (por el mismo token o porque la app es pÃºblica). "
                + reset_info
            )
            st.stop()
        except Exception as e:
            st.error(f"âš ï¸ Error inesperado al consultar X: {type(e).__name__}")
            st.stop()

        if response.data:
            # Mapa author_id -> objeto user
            users_by_id = {}
            if response.includes and "users" in response.includes:
                users_by_id = {u.id: u for u in response.includes["users"]}

            data = []
            for t in response.data:
                u = users_by_id.get(t.author_id)

                username = getattr(u, "username", None) if u else None
                name = getattr(u, "name", None) if u else None
                profile_location = getattr(u, "location", None) if u else None
                profile_desc = getattr(u, "description", None) if u else None

                ubicacion, confianza, fuente = infer_peru_location(profile_location, profile_desc)

                # Link pÃºblico al post (siempre que tengamos username)
                tweet_url = f"https://x.com/{username}/status/{t.id}" if username else ""

                data.append({
                    "Autor": f"@{username}" if username else (name or "Desconocido"),
                    "URL": tweet_url,
                    "Texto": t.text,
                    "Fecha": t.created_at,
                    "Likes": t.public_metrics.get("like_count", 0),
                    "Retweets": t.public_metrics.get("retweet_count", 0),
                    "UbicaciÃ³n inferida": ubicacion,
                    "Confianza": confianza,
                    "Fuente ubic.": fuente
                })

            df = pd.DataFrame(data)

            st.subheader("Resultados encontrados")
            st.caption("Nota: la ubicaciÃ³n NO es exacta; es una inferencia basada en 'location' del perfil y/o bio. Ãšsala solo como aproximaciÃ³n.")

            # Mostrar tabla
            st.dataframe(df, use_container_width=True)

            st.markdown("## ğŸ§  Resumen Ejecutivo AutomÃ¡tico")
            
            # --- PreparaciÃ³n de texto
            textos = df["Texto"].str.lower()
            
            # Stopwords bÃ¡sicas en espaÃ±ol (MVP)
            stopwords = set([
                "de","la","que","el","en","y","a","los","del","se","las","por","un","para","con",
                "no","una","su","al","lo","como","mÃ¡s","pero","sus","le","ya","o","este","sÃ­",
                "porque","esta","entre","cuando","muy","sin","sobre"
            ])
            
            def limpiar_texto(texto):
                palabras = re.findall(r"\b[a-zÃ¡Ã©Ã­Ã³ÃºÃ±]+\b", texto)
                return [p for p in palabras if p not in stopwords and len(p) > 3]
            
            # --- Narrativas dominantes
            todas_palabras = []
            for t in textos:
                todas_palabras.extend(limpiar_texto(t))
            
            top_palabras = pd.Series(todas_palabras).value_counts().head(10)
            
            # --- Sentimiento simple (lÃ©xico)
            positivas = set([
                # AprobaciÃ³n directa
                "bueno","bien","positivo","excelente","correcto","adecuado","acertado","justo",
                
                # Progreso / avance
                "avance","avanzar","mejora","mejorar","progreso","logro","logrado","resultado",
                
                # Confianza / esperanza
                "confianza","esperanza","optimismo","tranquilidad","seguridad","estabilidad",
                
                # GestiÃ³n / polÃ­tica pÃºblica
                "cumple","cumpliÃ³","eficiente","efectivo","funciona","soluciÃ³n","resuelve",
                
                # Legitimidad / respaldo
                "apoyo","respaldo","legÃ­timo","necesario","importante","prioritario",
                
                # Ã‰xito / impacto
                "exitoso","beneficio","beneficioso","impacto","positivo","histÃ³rico"
            ])
            
            negativas = set([
                # Rechazo directo
                "malo","mal","negativo","pÃ©simo","terrible","inaceptable","vergonzoso",
                
                # Crisis / conflicto
                "crisis","conflicto","caos","problema","grave","colapso","fracaso",
                
                # Desconfianza / enojo
                "indignaciÃ³n","enojo","rabia","molestia","hartazgo","descontento",
                
                # GestiÃ³n deficiente
                "ineficiente","incapaz","incompetente","error","fallo","improvisaciÃ³n",
                
                # CorrupciÃ³n / legitimidad
                "corrupciÃ³n","corrupto","ilegal","irregular","fraude","impunidad",
                
                # Miedo / riesgo
                "peligro","amenaza","riesgo","inseguridad","violencia","abuso",
                
                # Protesta / rechazo social
                "rechazo","repudio","protesta","denuncia","escÃ¡ndalo"
            ])
            
            def calcular_sentimiento(texto):
                palabras = limpiar_texto(texto)
                pos = sum(1 for p in palabras if p in positivas)
                neg = sum(1 for p in palabras if p in negativas)
                if pos > neg:
                    return "Positivo"
                if neg > pos:
                    return "Negativo"
                return "Neutral"
            
            # 1) Intentamos con Hugging Face (IA)
            sent_hf = []
            score_hf = []
            
            for txt in df["Texto"].tolist():
                s, sc = sentimiento_hf(txt)
                sent_hf.append(s)
                score_hf.append(sc)
            
            df["Sentimiento_HF"] = sent_hf
            df["Score_HF"] = score_hf
            
            # 2) Si Hugging Face falla, usamos el plan B (lÃ©xico)
            df["Sentimiento_Lex"] = df["Texto"].apply(calcular_sentimiento)
            
            # 3) Sentimiento final:
            # - Si HF dio respuesta: usamos HF
            # - Si HF no dio: usamos Lex
            df["Sentimiento"] = df["Sentimiento_HF"].fillna(df["Sentimiento_Lex"])

            # --- MÃ©tricas de temperatura
            total = len(df)
            pct_pos = round((df["Sentimiento"] == "Positivo").mean() * 100, 1)
            pct_neg = round((df["Sentimiento"] == "Negativo").mean() * 100, 1)
            pct_neu = round((df["Sentimiento"] == "Neutral").mean() * 100, 1)

            hf_ok = df["Sentimiento_HF"].notna().sum()
            if hf_ok > 0:
                metodo_sent = f"IA (Hugging Face) â€“ {modelo_hf_id}"
            else:
                metodo_sent = "LÃ©xico (fallback)"
            
            st.caption(f"MÃ©todo de sentimiento: {metodo_sent}. IA clasificÃ³ {hf_ok}/{len(df)} textos. Score HF â‰ˆ confianza (0â€“1).")

            if pct_neg > 40:
                temperatura = "ğŸ”´ Riesgo reputacional"
            elif pct_pos > 60:
                temperatura = "ğŸŸ¢ Clima favorable"
            else:
                temperatura = "ğŸŸ¡ Clima mixto / neutro"
            
            # --- Mostrar resumen ejecutivo
            st.markdown("### ğŸ“Œ Principales hallazgos")
            
            st.markdown(f"""
            - **Volumen analizado:** {total} publicaciones  
            - **Temperatura del tema:** {temperatura}  
            - **DistribuciÃ³n de sentimiento:**  
              - Positivo: {pct_pos}%  
              - Neutral: {pct_neu}%  
              - Negativo: {pct_neg}%  
            - **Narrativas dominantes:** {', '.join(top_palabras.index.tolist())}
            """)
            
            # --- Riesgos y oportunidades
            st.markdown("### âš ï¸ Riesgos identificados")
            if pct_neg > 30:
                st.markdown("- Presencia relevante de mensajes negativos que podrÃ­an escalar si aumenta el volumen.")
            else:
                st.markdown("- No se identifican riesgos reputacionales significativos en el periodo analizado.")
            
            st.markdown("### ğŸŒ± Oportunidades")
            if pct_pos > pct_neg:
                st.markdown("- Predominan mensajes favorables que pueden reforzarse con informaciÃ³n clara y oportuna.")
            else:
                st.markdown("- Existe oportunidad de clarificar informaciÃ³n y reducir ambigÃ¼edad en la conversaciÃ³n.")
            
            st.markdown("### ğŸ‘€ QuÃ© monitorear maÃ±ana")
            st.markdown("""
            - EvoluciÃ³n del volumen de publicaciones.
            - ApariciÃ³n de nuevos tÃ©rminos o hashtags.
            - Cambios en la proporciÃ³n de sentimiento negativo.
            - Mayor actividad desde regiones especÃ­ficas.
            """)
            
            st.markdown("### âš–ï¸ Advertencia metodolÃ³gica")
            st.caption(
                "Este anÃ¡lisis se basa en publicaciones pÃºblicas de X, con inferencia aproximada de ubicaciÃ³n "
                "y anÃ¡lisis automÃ¡tico de texto. No representa la opiniÃ³n de la totalidad de la poblaciÃ³n "
                "y debe interpretarse como una seÃ±al temprana, no como mediciÃ³n estadÃ­stica."
            )

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ğŸ“Š GRÃFICOS (Plotly) â€“ Dashboard Ejecutivo
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            
            st.markdown("## ğŸ“Š Tablero Visual")
            
            # Asegurar tipos
            df["Fecha"] = pd.to_datetime(df["Fecha"], errors="coerce")
            
            # Crear columna de dÃ­a para tendencias
            df["DÃ­a"] = df["Fecha"].dt.date.astype(str)
            
            # 1) Volumen por dÃ­a
            vol_por_dia = df.groupby("DÃ­a").size().reset_index(name="Volumen")
            
            fig_vol = px.line(
                vol_por_dia,
                x="DÃ­a",
                y="Volumen",
                markers=True,
                title="Volumen de publicaciones por dÃ­a"
            )
            st.plotly_chart(fig_vol, use_container_width=True)
            
            # 2) DistribuciÃ³n de sentimiento (donut)
            sent_counts = df["Sentimiento"].value_counts().reset_index()
            sent_counts.columns = ["Sentimiento", "Cantidad"]
            
            fig_sent = px.pie(
                sent_counts,
                names="Sentimiento",
                values="Cantidad",
                hole=0.45,
                title="DistribuciÃ³n de sentimiento (IA + fallback)"
            )
            st.plotly_chart(fig_sent, use_container_width=True)
            
            # 3) Sentimiento por dÃ­a (barras apiladas)
            sent_por_dia = df.groupby(["DÃ­a", "Sentimiento"]).size().reset_index(name="Cantidad")
            
            fig_sent_dia = px.bar(
                sent_por_dia,
                x="DÃ­a",
                y="Cantidad",
                color="Sentimiento",
                barmode="stack",
                title="Sentimiento por dÃ­a (barras apiladas)"
            )
            st.plotly_chart(fig_sent_dia, use_container_width=True)
            
            # 4) Top tÃ©rminos (narrativas dominantes)
            # Usamos tu funciÃ³n limpiar_texto y stopwords ya definidas arriba
            todas_palabras = []
            for t in df["Texto"].str.lower().tolist():
                todas_palabras.extend(limpiar_texto(t))
            
            top_terminos = pd.Series(todas_palabras).value_counts().head(15).reset_index()
            top_terminos.columns = ["TÃ©rmino", "Frecuencia"]
            
            fig_terms = px.bar(
                top_terminos,
                x="Frecuencia",
                y="TÃ©rmino",
                orientation="h",
                title="Top 15 tÃ©rminos dominantes (limpio de stopwords)"
            )
            st.plotly_chart(fig_terms, use_container_width=True)
            
            # 5) Top posts por interacciÃ³n (tabla)
            df["InteracciÃ³n"] = df["Likes"].fillna(0) + df["Retweets"].fillna(0)
            top_posts = df.sort_values("InteracciÃ³n", ascending=False).head(10)
            
            st.markdown("### ğŸ”¥ Top 10 posts por interacciÃ³n (Likes + Retweets)")
            st.dataframe(
                top_posts[["Autor", "Fecha", "Likes", "Retweets", "InteracciÃ³n", "Texto", "URL"]],
                use_container_width=True
            )



